{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import acquire\n",
    "import prepare\n",
    "\n",
    "from wrangle_zillow import wrangle_zillow_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('zillow_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal: Improve our original estimate of the log error by using clustering methodologies.\n",
    "\n",
    "## Acquisition, Prep, and Initial Exploration\n",
    "Using the notebook and files you created during the exercises make any changes, additions, etc. you want at this point. NOTE: You will NOT be splitting into train and test at this point.\n",
    "\n",
    "Ideas:\n",
    "\n",
    "   1. Data types:\n",
    "\n",
    "        - Write a function that takes in a dataframe and a list of column names and returns the dataframe with the datatypes of those columns changed to a non-numeric type.\n",
    "        - Use this function to appropriately transform any numeric columns that should not be treated as numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   2. Missing Values: Impute the values in land square feet.\n",
    "\n",
    "        - For land square feet, the goal is to impute the missing values by creating a linear model where landtaxvaluedollarcnt is the x-variable and the output/y-variable is the estimated land square feet.\n",
    "        - We'll then use this model to make predictions and fill in the missing values.\n",
    "        - Write a function that accepts the zillow data frame and returns the data frame with the missing values filled in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   3. Missing Values: Of the remaining missing values, can they be imputed or otherwise estimated?\n",
    "\n",
    "        - Impute those that can be imputed with the method you feel best fits the attribute.\n",
    "        - Decide whether to remove the rows or columns of any that cannot be reasonably imputed.\n",
    "        - Document your reasons for the decisions on how to handle each of those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   4. Outliers: Original from exercises. Adapt as you see fit.\n",
    "\n",
    "        - Write a function that accepts a series (i.e. one column from a data frame) and summarizes how many outliers are in the series. This function should accept a second parameter that determines how outliers are detected, with the ability to detect outliers in 3 ways: IQR, standard deviations (z-score), percentiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   5. Use your function defined above to identify columns where you should handle the outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   6. Write a function that accepts the zillow data frame and removes the outliers. You should make a decision and document how you will remove outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   7. Is there erroneous data you have found that you need to remove or repair? If so, take action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   8. Are there outliers you want to \"squeeze in\" to a max value? (e.g. all bathrooms > 6 => bathrooms = 6). If so, make those changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration with Clustering\n",
    "## Cluster the Target Variable\n",
    "    Why? By reducing the noise of the continuous variable, we can possibly see trends easier by turning this continuous variable into clusters and then comparing those clusters with respect to other variables through visualizations or tests.\n",
    "\n",
    "    Perform clustering with logerror as the only feature used in the clustering algorithm. Decide on a number of clusters to use, and store the cluster predictions back onto your data frame as cluster_target. Look at the centroids that were produced in this process. What do they tell you?\n",
    "\n",
    "    Use the produced clusters to help you explore through visualization how logerror relates to other variables. (A common way to do this is to use color to indicate the cluster id, and the other variables can be your x-axis and y-axis. (hint: look at your swarmplot function))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Independent Variables\n",
    "   You should also perform some clustering based on a number of independent variables. Create and evaluate several clustering models based on subsets of the independent variables. Here are some ideas:\n",
    "\n",
    "   - Location, that is, latitude and longitude\n",
    "   - Size (finished square feet)\n",
    "   - Location and size\n",
    "   - Be sure to use these new clusters in exploring your data, and interpret what these clusters tell you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Significance of Clusters\n",
    "    Use statistical testing methods to determine whether the clusters you have created are significant in terms of their relationship to logerror."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "## Feature Engineering\n",
    "   1. Remove variables that are not needed, wanted, useful, or are redundant.\n",
    "   2. Add any features you think may be useful.\n",
    "   3. Split your data into training and test sets.\n",
    "   4. Create subsets of data if you would like to create multiple models and then merge (such as, a different model for each cluster or for each county)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "   1. Train at least 3 different models (a model is different if there are changes in one or more of the following: features, hyper-parameters, algorithm). Create object, fit, predict & evaluate. Use mean absolute error or mean squared error to evaluate. Also, try regression algorithms you have not used before.\n",
    "   2. Evaluate your best model on your test data set to get an idea of your model's out of sample error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
