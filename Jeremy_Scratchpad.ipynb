{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (wrangle_zillow.py, line 18)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/usr/local/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3326\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-57d47183437e>\"\u001b[0;36m, line \u001b[0;32m24\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from wrangle_zillow import wrangle_zillow_data\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/jivemachine/codeup-data-science/Zillow-Clustering/wrangle_zillow.py\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import acquire\n",
    "import prepare\n",
    "import env\n",
    "\n",
    "from wrangle_zillow import wrangle_zillow_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query ='''\n",
    "select \n",
    "    prop.parcelid\n",
    "    , pred.logerror\n",
    "    , pred.transactiondate\n",
    "    , bathroomcnt\n",
    "    , bedroomcnt\n",
    "    , calculatedfinishedsquarefeet\n",
    "    , fips\n",
    "    , latitude\n",
    "    , longitude\n",
    "    , lotsizesquarefeet\n",
    "    , regionidcity\n",
    "    , regionidcounty\n",
    "    , regionidneighborhood\n",
    "    , regionidzip\n",
    "    , yearbuilt\n",
    "    , structuretaxvaluedollarcnt\n",
    "    , taxvaluedollarcnt\n",
    "    , landtaxvaluedollarcnt\n",
    "    , taxamount\n",
    "from properties_2017 prop\n",
    "inner join predictions_2017 pred on prop.parcelid = pred.parcelid\n",
    "where propertylandusetypeid = 261;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, env.get_url('zillow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('zillow_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal: Improve our original estimate of the log error by using clustering methodologies.\n",
    "\n",
    "## Acquisition, Prep, and Initial Exploration\n",
    "Using the notebook and files you created during the exercises make any changes, additions, etc. you want at this point. NOTE: You will NOT be splitting into train and test at this point.\n",
    "\n",
    "Ideas:\n",
    "\n",
    "   1. Data types:\n",
    "\n",
    "        - Write a function that takes in a dataframe and a list of column names and returns the dataframe with the datatypes of those columns changed to a non-numeric type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it works!\n",
    "cols = ['bathroomcnt', 'latitude']\n",
    "def change_data(df, cols):\n",
    "    \"\"\"\n",
    "    takes a dataframe and a list of columns and it \n",
    "    converts the columns listed that are in the dataframe into \n",
    "    objects in the same dataframe\n",
    "    \"\"\"\n",
    "    newdf = pd.DataFrame(df, columns=cols)\n",
    "    df = df.drop(columns=cols)\n",
    "    newdf = newdf.astype(object)\n",
    "    df = pd.concat([df, newdf], axis=1)\n",
    "    return df\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "#zillow = df.copy()\n",
    "#zillow = zillow.astype(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - Use this function to appropriately transform any numeric columns that should not be treated as numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">upon more inspection it looks like we could use this function to convert zipcodes parcelid id fips, rawcensustractbloc, regionid's,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['parcelid','fips', 'regionidcity', 'regionidcounty', 'regionidzip',\n",
    "       'regionidneighborhood',]\n",
    "zillow = change_data(df, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   2. Missing Values: Impute the values in land square feet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(zillow.isna().sum()/len(zillow)*100,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   3. Missing Values: Of the remaining missing values, can they be imputed or otherwise estimated?\n",
    "\n",
    "        - Impute those that can be imputed with the method you feel best fits the attribute.\n",
    "        - Decide whether to remove the rows or columns of any that cannot be reasonably imputed.\n",
    "        - Document your reasons for the decisions on how to handle each of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(zillow.isna().sum()/len(zillow)*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're going to drop regionidneighborhood because it's missing over 50% of it's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow = zillow.drop(columns=['regionidneighborhood'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're going to check out regionidcity and see if that column is vital to our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow.regionidcity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm not sure what these codes are, so we're going to drop this \n",
    "#columns for now and if we need it later i'll impute\n",
    "zillow = zillow.drop(columns=['regionidcity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to use the median to fill taxamount\n",
    "zillow['taxamount'] = zillow['taxamount'].fillna(zillow.taxamount.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna's with median for yearbuilt\n",
    "zillow['yearbuilt'] = zillow['yearbuilt'].fillna(zillow.yearbuilt.median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna's with median for structuretaxvaluedollarcnt\n",
    "zillow['structuretaxvaluedollarcnt'] = zillow['structuretaxvaluedollarcnt'].fillna(zillow.structuretaxvaluedollarcnt.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna's with median for calculatedfinishedsquarefeet\n",
    "zillow['calculatedfinishedsquarefeet'] = zillow['calculatedfinishedsquarefeet'].fillna(zillow.calculatedfinishedsquarefeet.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow['regionidzip'] = zillow['regionidzip'].fillna(zillow.regionidzip.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding median value to those 2 missing rows in landtax and taxvalue & lotsizesquarefeet\n",
    "zillow['taxvaluedollarcnt'] = zillow['taxvaluedollarcnt'].fillna(zillow.taxvaluedollarcnt.median())\n",
    "zillow['landtaxvaluedollarcnt'] = zillow['landtaxvaluedollarcnt'].fillna(zillow.landtaxvaluedollarcnt.median())\n",
    "zillow['lotsizesquarefeet'] = zillow['lotsizesquarefeet'].fillna(zillow.lotsizesquarefeet.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def the_master_imputer(df):\n",
    "    for col in df:\n",
    "        if col.isna().sum() > 0:\n",
    "            df[f'{col}'] = df[f'{col}'].fillna(df.col.median())\n",
    "    return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_data_to_int(df, cols):\n",
    "    \"\"\"\n",
    "    takes a dataframe and a list of columns and it \n",
    "    converts the columns listed that are in the dataframe into \n",
    "    objects in the same dataframe\n",
    "    \"\"\"\n",
    "    newdf = pd.DataFrame(df, columns=cols)\n",
    "    df = df.drop(columns=cols)\n",
    "    newdf = newdf.astype(int)\n",
    "    df = pd.concat([df, newdf], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning our df into a csv a\n",
    "zillow.to_csv('zillow.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow = pd.read_csv('zillow.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow = zillow.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're going to convert id's and zip codes into objects\n",
    "cols = ['parcelid','fips', 'regionidcounty', 'regionidzip']\n",
    "zillow = change_data(zillow, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   4. Outliers: Original from exercises. Adapt as you see fit.\n",
    "\n",
    "        - Write a function that accepts a series (i.e. one column from a data frame) and summarizes how many outliers are in the series. This function should accept a second parameter that determines how outliers are detected, with the ability to detect outliers in 3 ways: IQR, standard deviations (z-score), percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts df to series\n",
    "def convert_to_series(df):\n",
    "    '''\n",
    "    helper function for the summarize function\n",
    "    that converts a dataframe into a series and grabs \n",
    "    the value counts from that dataframe\n",
    "    '''\n",
    "    series = pd.Series([])\n",
    "    for _, col in enumerate(df.columns.values):\n",
    "        if df[col].dtype == 'object':\n",
    "            col_count = df[col].value_counts()\n",
    "        else:\n",
    "            col_count = df[col].value_counts(bins=10)\n",
    "        series = series.append(col_count)\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier detection with IQR as a filter\n",
    "def get_upper_outliers(s, k):\n",
    "    '''\n",
    "    Given a series and a cutoff value, k, returns the upper outliers for the\n",
    "    series.\n",
    "\n",
    "    The values returned will be either 0 (if the point is not an outlier), or a\n",
    "    number that indicates how far away from the upper bound the observation is.\n",
    "    '''\n",
    "    q1, q3 = s.quantile([.25, .75])\n",
    "    iqr = q3 - q1\n",
    "    upper_bound = q3 + k * iqr\n",
    "    return s.apply(lambda x: max([x - upper_bound, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_upper_outlier_columns(df, k):\n",
    "    '''\n",
    "    Add a column with the suffix _outliers for all the numeric columns\n",
    "    in the given dataframe.\n",
    "    '''\n",
    "    # outlier_cols = {col + '_outliers': get_upper_outliers(df[col], k)\n",
    "    #                 for col in df.select_dtypes('number')}\n",
    "    # return df.assign(**outlier_cols)\n",
    "\n",
    "    for col in df.select_dtypes('number'):\n",
    "        df[col + '_outliers'] = get_upper_outliers(df[col], k)\n",
    "\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_upper_outlier_columns(zillow, 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_cols = [col for col in zillow if col.endswith('_outliers')]\n",
    "for col in outlier_cols:\n",
    "    print('~~~\\n' + col)\n",
    "    data = zillow[col][zillow[col] > 0]\n",
    "    print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're going to drop the rows of houses worth over 1million dollars\n",
    "zillow[zillow['structuretaxvaluedollarcnt'] > 1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow.drop(zillow[zillow.structuretaxvaluedollarcnt >= 1_000_000].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   5. Use your function defined above to identify columns where you should handle the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = pd.DataFrame()\n",
    "if col in df.corr() >= 0.5:\n",
    "    print(\"got it\")\n",
    "else:\n",
    "    print(\"not it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   6. Write a function that accepts the zillow data frame and removes the outliers. You should make a decision and document how you will remove outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   7. Is there erroneous data you have found that you need to remove or repair? If so, take action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   8. Are there outliers you want to \"squeeze in\" to a max value? (e.g. all bathrooms > 6 => bathrooms = 6). If so, make those changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration with Clustering\n",
    "## Cluster the Target Variable\n",
    "    Why? By reducing the noise of the continuous variable, we can possibly see trends easier by turning this continuous variable into clusters and then comparing those clusters with respect to other variables through visualizations or tests.\n",
    "\n",
    "    Perform clustering with logerror as the only feature used in the clustering algorithm. Decide on a number of clusters to use, and store the cluster predictions back onto your data frame as cluster_target. Look at the centroids that were produced in this process. What do they tell you?\n",
    "\n",
    "    Use the produced clusters to help you explore through visualization how logerror relates to other variables. (A common way to do this is to use color to indicate the cluster id, and the other variables can be your x-axis and y-axis. (hint: look at your swarmplot function))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Independent Variables\n",
    "   You should also perform some clustering based on a number of independent variables. Create and evaluate several clustering models based on subsets of the independent variables. Here are some ideas:\n",
    "\n",
    "   - Location, that is, latitude and longitude\n",
    "   - Size (finished square feet)\n",
    "   - Location and size\n",
    "   - Be sure to use these new clusters in exploring your data, and interpret what these clusters tell you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Significance of Clusters\n",
    "    Use statistical testing methods to determine whether the clusters you have created are significant in terms of their relationship to logerror."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "## Feature Engineering\n",
    "   1. Remove variables that are not needed, wanted, useful, or are redundant.\n",
    "   2. Add any features you think may be useful.\n",
    "   3. Split your data into training and test sets.\n",
    "   4. Create subsets of data if you would like to create multiple models and then merge (such as, a different model for each cluster or for each county)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "   1. Train at least 3 different models (a model is different if there are changes in one or more of the following: features, hyper-parameters, algorithm). Create object, fit, predict & evaluate. Use mean absolute error or mean squared error to evaluate. Also, try regression algorithms you have not used before.\n",
    "   2. Evaluate your best model on your test data set to get an idea of your model's out of sample error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
