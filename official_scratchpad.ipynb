{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from acquire import acquire_data\n",
    "from prepare import the_master_imputer, data_prep, percent_of_values_missing, change_data_to_int, change_data_to_object\n",
    "from set_counties import create_county_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- using our function from our acquire.py file to import our data from SQL and import it into a csv, drop the Unnamed column and assign it to a data frame\n",
    "- we also cut outliers but removing any home where their structuretaxvaluedollarcnt was > $1,000,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquiring data ...\n",
      "\n",
      "- csv already exist\n",
      "\n",
      "Data has been acquired\n"
     ]
    }
   ],
   "source": [
    "df = acquire_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- our data_prep function removes any column missing 50% or more of their data as well as any row missing up to 75% of it's data (*We didn't want to waste time strategizing ways to accuratly sum up data missing over 50% of it's values*) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_prep(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the function below tells us what percent of each column is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parcelid                        0.00\n",
       "logerror                        0.00\n",
       "transactiondate                 0.00\n",
       "bathroomcnt                     0.00\n",
       "bedroomcnt                      0.00\n",
       "calculatedfinishedsquarefeet    0.10\n",
       "fips                            0.00\n",
       "latitude                        0.00\n",
       "longitude                       0.00\n",
       "lotsizesquarefeet               0.65\n",
       "regionidcity                    1.96\n",
       "regionidcounty                  0.00\n",
       "regionidzip                     0.04\n",
       "yearbuilt                       0.16\n",
       "structuretaxvaluedollarcnt      0.00\n",
       "taxvaluedollarcnt               0.00\n",
       "landtaxvaluedollarcnt           0.00\n",
       "taxamount                       0.01\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_of_values_missing(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- using the_master_imputer function, we will fill all of our missing functions with the median value, for that column's rows (*Since we weren't missing more than 2% of any data, we felt that the median would suffice for the missing values in our dataset*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = the_master_imputer(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that all of our missing values are taken care of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parcelid                        0\n",
       "logerror                        0\n",
       "transactiondate                 0\n",
       "bathroomcnt                     0\n",
       "bedroomcnt                      0\n",
       "calculatedfinishedsquarefeet    0\n",
       "fips                            0\n",
       "latitude                        0\n",
       "longitude                       0\n",
       "lotsizesquarefeet               0\n",
       "regionidcity                    0\n",
       "regionidcounty                  0\n",
       "regionidzip                     0\n",
       "yearbuilt                       0\n",
       "structuretaxvaluedollarcnt      0\n",
       "taxvaluedollarcnt               0\n",
       "landtaxvaluedollarcnt           0\n",
       "taxamount                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">now we're going to take our fips # and use that to impute what county our observations were made in using our create_county_cols function and in the process we're going to drop our fips & our regioncounty columns.\n",
    "fips source: [THIS LINK](https://www.nrcs.usda.gov/wps/portal/nrcs/detail/?cid=nrcs143_013697)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_county_cols(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets change our values so we can have less noise in our exploration / modeling phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- using our change_data_to_object function to change the datatypes of *parcelid*, *regionidcity*, & *regionidzip* to object. (we dont want to do any addition with our unit id's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['parcelid', 'regionidzip', 'regionidcity']\n",
    "df = change_data_to_object(df, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- using our change_data_to_int function to change the datatypes of *yearbuilt*, *latitude*, *longitude*, *lotsizesquarefeet*, *calculatedfinishedsquarefeet*, & *bedroomcnt* to change these datatypes from float's to integers to make the data more exclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['yearbuilt', 'latitude', 'longitude', 'lotsizesquarefeet', 'calculatedfinishedsquarefeet', 'bedroomcnt' ]\n",
    "df = change_data_to_int(df, cols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
